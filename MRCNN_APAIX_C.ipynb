{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUnt6ZLX_RR6",
    "outputId": "02f07cfc-62e7-419d-dc69-bee9662f2a1e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/z-mahmud22/Mask-RCNN_TF2.14.0.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgxnmkomAeBY",
    "outputId": "0a866e6b-5ba4-459d-e59b-e562ba80f136"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "Hei7nkECAliP",
    "outputId": "d7dd3893-b7d4-4389-bb6a-8656f6f8de28"
   },
   "outputs": [],
   "source": [
    "# installation du requirement.text\n",
    "!pip install cython==3.0.5 h5py==3.9.0 imgaug==0.4.0 ipython==7.34.0 ipython-genutils==0.2.0 ipython-sql==0.5.0 keras==2.14.0 matplotlib==3.7.1 numpy==1.23.5 opencv-contrib-python==4.8.0.76 opencv-python==4.8.0.76 pillow==9.4.0 scikit-image==0.19.3 scipy==1.11.3 tensorboard==2.14.1 tensorflow[and-cuda]==2.14.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "X4G52rrkLbx2",
    "outputId": "fea9c30b-e9e9-49b8-d931-c74004e9b8ff"
   },
   "outputs": [],
   "source": [
    "# Se déplacer vers le répertoire contenant setup.py\n",
    "%cd /content/Mask-RCNN_TF2.14.0\n",
    "\n",
    "# Installer le package et ses dépendances\n",
    "!pip install .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1_R3ZDSLHyW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# ici mon chemin d'accès est celui de colab où j'importe mes images et annotations\n",
    "data_dir = '/content'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importation des poids de coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJZPREtARFb6",
    "outputId": "dadb3944-f3e3-4da0-fe2d-c71086b3fc93"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7MBACryl5ha"
   },
   "outputs": [],
   "source": [
    "#première config\n",
    "from mrcnn.config import Config\n",
    "\n",
    "class CustomConfig(Config):\n",
    "    NAME = \"baby\"\n",
    "    IMAGES_PER_GPU = 2\n",
    "    NUM_CLASSES = 1 + 2  # Background + \"nose\" + \"face\"\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    VALIDATION_STEPS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    # Dimensions des images\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    IMAGE_SHAPE = [512, 512, 3]\n",
    "\n",
    "    # Utilisation d'un mini masque pour économiser de la mémoire\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)\n",
    "\n",
    "    # Paramètres du RPN (Region Proposal Network)\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "    GPU_COUNT = 1\n",
    "# Instantiate your custom configuration\n",
    "config = CustomConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cst8XFsybkKW"
   },
   "source": [
    "Passage des annotations au bon format (les deux formats sont disponibles dans mon repo Git donc vous pouvez sauter cette étape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQcV5j1XNzcn",
    "outputId": "b7f24be2-9755-43ad-c6d6-3f98f5e614eb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Charger les données d'entrée depuis votre chemin\n",
    "input_path = \"/content/images/train/train_annotations_json.json\"\n",
    "output_path = \"/content/images/train/train_annotations_coco.json\"\n",
    "\n",
    "# Charger les données d'entrée\n",
    "with open(input_path, \"r\") as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "# Initialiser les structures pour COCO\n",
    "coco_format = {\n",
    "    \"info\": {\n",
    "        \"year\": 2024,\n",
    "        \"version\": \"1.0\",\n",
    "        \"description\": \"Dataset COCO pour Baby Dataset\",\n",
    "        \"contributor\": \"\",\n",
    "        \"url\": \"\",\n",
    "        \"date_created\": \"2024-11-17\"\n",
    "    },\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"nose\", \"supercategory\": \"baby\"},\n",
    "        {\"id\": 2, \"name\": \"face\", \"supercategory\": \"baby\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Variables auxiliaires\n",
    "image_id = 1\n",
    "annotation_id = 1\n",
    "category_map = {\"nose\": 1, \"face\": 2}\n",
    "\n",
    "# Parcourir les images\n",
    "for image_key, image_data in input_data.items():\n",
    "    # Ajouter les métadonnées de l'image\n",
    "    coco_format[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"width\": 640,  # Remplacer par la largeur réelle si disponible\n",
    "        \"height\": 640,  # Remplacer par la hauteur réelle si disponible\n",
    "        \"file_name\": image_data[\"filename\"],\n",
    "        \"license\": 0,\n",
    "        \"date_captured\": \"\"\n",
    "    })\n",
    "\n",
    "    # Ajouter les annotations\n",
    "    for region in image_data[\"regions\"]:\n",
    "        shape = region[\"shape_attributes\"]\n",
    "        attributes = region[\"region_attributes\"]\n",
    "        category_name = list(attributes.values())[0]  # Exemple : \"nose\" ou \"face\"\n",
    "\n",
    "        if category_name in category_map:\n",
    "            category_id = category_map[category_name]\n",
    "            x, y, width, height = shape[\"x\"], shape[\"y\"], shape[\"width\"], shape[\"height\"]\n",
    "\n",
    "            # Calcul du segment\n",
    "            segmentation = [\n",
    "                [x, y, x + width, y, x + width, y + height, x, y + height]\n",
    "            ]\n",
    "\n",
    "            # Ajouter l'annotation\n",
    "            coco_format[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, width, height],\n",
    "                \"area\": width * height,\n",
    "                \"iscrowd\": 0,\n",
    "                \"segmentation\": segmentation\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    image_id += 1\n",
    "\n",
    "# Sauvegarder dans un fichier JSON au format COCO\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    json.dump(coco_format, output_file, indent=4)\n",
    "\n",
    "print(f\"Fichier converti enregistré à : {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qV7WAROMXjM",
    "outputId": "f06be21a-2f27-4875-d5c1-c28076583495"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Charger les données d'entrée depuis votre chemin\n",
    "input_path = \"/content/images/test/test_annotations_json.json\"\n",
    "\n",
    "output_path = \"/content/images/test/test_annotations_coco.json\"\n",
    "\n",
    "# Charger les données d'entrée\n",
    "with open(input_path, \"r\") as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "# Initialiser les structures pour COCO\n",
    "coco_format = {\n",
    "    \"info\": {\n",
    "        \"year\": 2024,\n",
    "        \"version\": \"1.0\",\n",
    "        \"description\": \"Dataset COCO pour Baby Dataset\",\n",
    "        \"contributor\": \"\",\n",
    "        \"url\": \"\",\n",
    "        \"date_created\": \"2024-11-17\"\n",
    "    },\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"nose\", \"supercategory\": \"baby\"},\n",
    "        {\"id\": 2, \"name\": \"face\", \"supercategory\": \"baby\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Variables auxiliaires\n",
    "image_id = 1\n",
    "annotation_id = 1\n",
    "category_map = {\"nose\": 1, \"face\": 2}\n",
    "\n",
    "# Parcourir les images\n",
    "for image_key, image_data in input_data.items():\n",
    "    # Ajouter les métadonnées de l'image\n",
    "    coco_format[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"width\": 640,  # Remplacer par la largeur réelle si disponible\n",
    "        \"height\": 640,  # Remplacer par la hauteur réelle si disponible\n",
    "        \"file_name\": image_data[\"filename\"],\n",
    "        \"license\": 0,\n",
    "        \"date_captured\": \"\"\n",
    "    })\n",
    "\n",
    "    # Ajouter les annotations\n",
    "    for region in image_data[\"regions\"]:\n",
    "        shape = region[\"shape_attributes\"]\n",
    "        attributes = region[\"region_attributes\"]\n",
    "        category_name = list(attributes.values())[0]  # Exemple : \"nose\" ou \"face\"\n",
    "\n",
    "        if category_name in category_map:\n",
    "            category_id = category_map[category_name]\n",
    "            x, y, width, height = shape[\"x\"], shape[\"y\"], shape[\"width\"], shape[\"height\"]\n",
    "\n",
    "            # Calcul du segment\n",
    "            segmentation = [\n",
    "                [x, y, x + width, y, x + width, y + height, x, y + height]\n",
    "            ]\n",
    "\n",
    "            # Ajouter l'annotation\n",
    "            coco_format[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, width, height],\n",
    "                \"area\": width * height,\n",
    "                \"iscrowd\": 0,\n",
    "                \"segmentation\": segmentation\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    image_id += 1\n",
    "\n",
    "# Sauvegarder dans un fichier JSON au format COCO\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    json.dump(coco_format, output_file, indent=4)\n",
    "\n",
    "print(f\"Fichier converti enregistré à : {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRjiGVtTMYhH",
    "outputId": "5d632e37-2e06-4249-f0ee-9b28d00e6587"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Charger les données d'entrée depuis votre chemin\n",
    "input_path = \"/content/images/valid/valid_annotations_json.json\"\n",
    "output_path = \"/content/images/valid/valid_annotations_coco.json\"\n",
    "\n",
    "# Charger les données d'entrée\n",
    "with open(input_path, \"r\") as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "# Initialiser les structures pour COCO\n",
    "coco_format = {\n",
    "    \"info\": {\n",
    "        \"year\": 2024,\n",
    "        \"version\": \"1.0\",\n",
    "        \"description\": \"Dataset COCO pour Baby Dataset\",\n",
    "        \"contributor\": \"\",\n",
    "        \"url\": \"\",\n",
    "        \"date_created\": \"2024-11-17\"\n",
    "    },\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"nose\", \"supercategory\": \"baby\"},\n",
    "        {\"id\": 2, \"name\": \"face\", \"supercategory\": \"baby\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Variables auxiliaires\n",
    "image_id = 1\n",
    "annotation_id = 1\n",
    "category_map = {\"nose\": 1, \"face\": 2}\n",
    "\n",
    "# Parcourir les images\n",
    "for image_key, image_data in input_data.items():\n",
    "    # Ajouter les métadonnées de l'image\n",
    "    coco_format[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"width\": 640,  # Remplacer par la largeur réelle si disponible\n",
    "        \"height\": 640,  # Remplacer par la hauteur réelle si disponible\n",
    "        \"file_name\": image_data[\"filename\"],\n",
    "        \"license\": 0,\n",
    "        \"date_captured\": \"\"\n",
    "    })\n",
    "\n",
    "    # Ajouter les annotations\n",
    "    for region in image_data[\"regions\"]:\n",
    "        shape = region[\"shape_attributes\"]\n",
    "        attributes = region[\"region_attributes\"]\n",
    "        category_name = list(attributes.values())[0]  # Exemple : \"nose\" ou \"face\"\n",
    "\n",
    "        if category_name in category_map:\n",
    "            category_id = category_map[category_name]\n",
    "            x, y, width, height = shape[\"x\"], shape[\"y\"], shape[\"width\"], shape[\"height\"]\n",
    "\n",
    "            # Calcul du segment\n",
    "            segmentation = [\n",
    "                [x, y, x + width, y, x + width, y + height, x, y + height]\n",
    "            ]\n",
    "\n",
    "            # Ajouter l'annotation\n",
    "            coco_format[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, width, height],\n",
    "                \"area\": width * height,\n",
    "                \"iscrowd\": 0,\n",
    "                \"segmentation\": segmentation\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    image_id += 1\n",
    "\n",
    "# Sauvegarder dans un fichier JSON au format COCO\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    json.dump(coco_format, output_file, indent=4)\n",
    "\n",
    "print(f\"Fichier converti enregistré à : {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G4Tq1Tlbz9r"
   },
   "source": [
    "Définition des fonctions principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mc0AA4OOrDIp",
    "outputId": "250f1045-1cf5-4d09-aa0d-72abd8cb3bae"
   },
   "outputs": [],
   "source": [
    "from mrcnn.utils import Dataset\n",
    "from mrcnn.model import *\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from pycocotools import mask as maskUtils\n",
    "config = CustomConfig()\n",
    "\n",
    "class BabyDataset(Dataset):\n",
    "    def load_baby(self, dataset_dir, subset):\n",
    "        \"\"\"Charge le dataset de bébés à partir du répertoire spécifié.\"\"\"\n",
    "        assert subset in [\"train\", \"valid\",\"test\"]\n",
    "        dataset_path = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Charger le fichier d'annotations approprié\n",
    "        annotations_file = os.path.join(dataset_path, f\"{subset}_annotations_coco.json\")\n",
    "        if not os.path.exists(annotations_file):\n",
    "            raise FileNotFoundError(f\"Le fichier d'annotations {annotations_file} est introuvable.\")\n",
    "\n",
    "        # Charger les annotations avec COCO directement à partir du chemin du fichier\n",
    "        coco = COCO(annotations_file)\n",
    "\n",
    "        # Ajouter des classes (basé sur le fichier d'annotations)\n",
    "        for cat in coco.loadCats(coco.getCatIds()):\n",
    "            self.add_class(\"baby\", cat['id'], cat['name'])\n",
    "\n",
    "        # Ajouter les images\n",
    "        image_ids = list(coco.imgs.keys())\n",
    "        for image_id in image_ids:\n",
    "            image_info = coco.loadImgs(image_id)[0]\n",
    "            image_path = os.path.join(dataset_path, image_info['file_name'])\n",
    "\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image {image_path} manquante.\")\n",
    "                continue\n",
    "\n",
    "            # Ajouter l'image au dataset\n",
    "            self.add_image(\n",
    "                \"baby\",\n",
    "                image_id=image_id,\n",
    "                path=image_path,\n",
    "                width=image_info['width'],\n",
    "                height=image_info['height'],\n",
    "                annotations_path=annotations_file\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "      \"\"\"Génère les masques pour une image donnée.\"\"\"\n",
    "      image_info = self.image_info[image_id]\n",
    "      coco = COCO(image_info['annotations_path'])\n",
    "      annotations = coco.loadAnns(coco.getAnnIds(imgIds=image_info['id'], iscrowd=False))\n",
    "\n",
    "      height, width = image_info['height'], image_info['width']\n",
    "\n",
    "      # Initialiser le masque avec le bon type de données\n",
    "      mask = np.zeros((config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], len(annotations)), dtype=np.uint8)  # Changed to config.IMAGE_SHAPE\n",
    "      class_ids = []\n",
    "\n",
    "      for i, ann in enumerate(annotations):\n",
    "          rle = coco.annToRLE(ann)\n",
    "          m = maskUtils.decode(rle)\n",
    "\n",
    "          # Vérifier le type du masque après le décodage\n",
    "          assert m.dtype == np.uint8 or m.dtype == np.bool_, f\"Mask dtype after decode is {m.dtype}\"\n",
    "\n",
    "          # Redimensionner le masque pour qu'il corresponde à la taille de l'image\n",
    "          if m.shape[0] != height or m.shape[1] != width:\n",
    "              m = cv2.resize(m, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "          # Redimensionner le masque pour qu'il corresponde aux dimensions attendues par le modèle\n",
    "          m = cv2.resize(m, (config.IMAGE_SHAPE[1], config.IMAGE_SHAPE[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "          # Vérifier le type du masque après le redimensionnement\n",
    "          assert m.dtype == np.uint8, f\"Mask dtype after resize is {m.dtype}\"\n",
    "\n",
    "          # Convertir en uint8 si nécessaire\n",
    "          mask[:, :, i] = m.astype(np.uint8)\n",
    "\n",
    "          # Vérifier le type du masque après la conversion\n",
    "          assert mask[:, :, i].dtype == np.uint8, f\"Mask dtype after assignment is {mask[:, :, i].dtype}\"\n",
    "\n",
    "          # Ajouter la classe correspondante\n",
    "          class_id = self.map_source_class_id(f\"baby.{ann['category_id']}\")\n",
    "          class_ids.append(class_id)\n",
    "\n",
    "      # Move the conversion to NumPy array outside the loop\n",
    "      class_ids = np.array(class_ids, dtype=np.int32)\n",
    "      return mask, class_ids\n",
    "\n",
    "\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Retourne le chemin de l'image.\"\"\"\n",
    "        return self.image_info[image_id][\"path\"]\n",
    "\n",
    "# Charger le dataset d'entraînement\n",
    "dataset_dir = \"/content/images\"\n",
    "dataset_train = BabyDataset()\n",
    "dataset_train.load_baby(dataset_dir, 'train')\n",
    "dataset_train.prepare()\n",
    "print(f\"Nombre d'images dans le dataset d'entraînement: {len(dataset_train.image_ids)}\")\n",
    "\n",
    "# Charger le dataset de validation\n",
    "dataset_val = BabyDataset()\n",
    "dataset_val.load_baby(dataset_dir, 'valid')\n",
    "dataset_val.prepare()\n",
    "print(f\"Nombre d'images dans le dataset de validation: {len(dataset_val.image_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8oSaIDwomUR",
    "outputId": "c811e87e-2354-40ff-8ef0-09a896429aae"
   },
   "outputs": [],
   "source": [
    "# Tester le chargement du masque pour une image\n",
    "image_id = dataset_train.image_ids[0]\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Classes: {class_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R4zNyJrb6rD"
   },
   "source": [
    "verification importation image et masque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "id": "8m512VldlemR",
    "outputId": "48ce1818-d7f3-4370-a0cf-adc784f76298"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import random\n",
    "\n",
    "def display_image_with_masks(image, mask, class_ids, class_names):\n",
    "    \"\"\"\n",
    "    Affiche l'image avec les masques superposés et les classes annotées.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Générer des couleurs aléatoires pour chaque classe\n",
    "    colors = [(random.random(), random.random(), random.random()) for _ in range(len(class_ids))]\n",
    "\n",
    "    # Superposer chaque masque sur l'image\n",
    "    for i in range(mask.shape[-1]):\n",
    "        m = mask[:, :, i]\n",
    "        color = colors[i]\n",
    "\n",
    "        # Superposer le masque en semi-transparent\n",
    "        plt.imshow(m, cmap='cool', alpha=0.4)\n",
    "\n",
    "        # Ajouter une légende avec le nom de la classe\n",
    "        class_id = class_ids[i]\n",
    "        class_name = class_names[class_id]\n",
    "        plt.text(10, 30 + i * 20, f\"{class_name} (ID {class_id})\", color=color, fontsize=12, backgroundcolor='black')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Charger une image et ses masques\n",
    "image_id = dataset_train.image_ids[0]\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "\n",
    "# Afficher l'image avec les masques superposés\n",
    "display_image_with_masks(image, mask, class_ids, [\"BG\", \"nose\", \"face\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zYZ8EhcmBe6",
    "outputId": "99211b7c-400b-4a7e-eea5-2d1eec7c1255"
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"/content/images\"\n",
    "\n",
    "# Charger le dataset d'entraînement\n",
    "dataset_train = BabyDataset()\n",
    "dataset_train.load_baby(dataset_dir, 'train')\n",
    "dataset_train.prepare()\n",
    "print(f\"Nombre d'images dans le dataset d'entraînement: {len(dataset_train.image_ids)}\")\n",
    "# Charger le dataset de validation\n",
    "dataset_val = BabyDataset()\n",
    "dataset_val.load_baby(dataset_dir, 'valid')\n",
    "dataset_val.prepare()\n",
    "print(f\"Nombre d'images dans le dataset de validation: {len(dataset_val.image_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P220ccmncL-S"
   },
   "source": [
    "A partir d'ici tout est en double car je teste deux configuration des hyperparamètres afin de voir lesquels sont meilleurs pour mon model avec mon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvNobOddvL_4"
   },
   "outputs": [],
   "source": [
    "class BabyConfig(Config):\n",
    "    NAME = \"baby\"\n",
    "    NUM_CLASSES = 1 + 2  # Fond + nose et face\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    VALIDATION_STEPS = 10\n",
    "    IMAGE_MIN_DIM = 640\n",
    "    IMAGE_MAX_DIM = 640\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "    MAX_GT_INSTANCES = 50\n",
    "    DETECTION_MAX_INSTANCES = 50\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8OO4SHLpSDy"
   },
   "outputs": [],
   "source": [
    "class BabyConfig_1(Config):\n",
    "    NAME = \"baby\"\n",
    "    NUM_CLASSES = 1 + 2  # Fond + nose et face\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    VALIDATION_STEPS = 10\n",
    "    IMAGE_MIN_DIM = 640\n",
    "    IMAGE_MAX_DIM = 640\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "    MAX_GT_INSTANCES = 50\n",
    "    DETECTION_MAX_INSTANCES = 50\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "    LEARNING_RATE = 0.001  # Taux d'apprentissage initial\n",
    "    # Taille des lots\n",
    "    BATCH_SIZE = GPU_COUNT * IMAGES_PER_GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDJCUYP5chKV"
   },
   "source": [
    "création du model et entrainement avec la première configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkb4kGChmEdR",
    "outputId": "51fb52a4-e427-4ccb-f0e0-e198d3ab0710"
   },
   "outputs": [],
   "source": [
    "from mrcnn.model import MaskRCNN\n",
    "config = BabyConfig()\n",
    "# Créer le modèle en mode entraînement\n",
    "model = MaskRCNN(mode=\"training\", config=config, model_dir='/content/logs')\n",
    "\n",
    "# Charger un modèle pré-entraîné (par exemple COCO)\n",
    "model.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=[\n",
    "    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "# Vérifier les types de données avant l'entraînement\n",
    "for image_id in dataset_train.image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    assert image.dtype == np.uint8, f\"Training image dtype is {image.dtype}\"\n",
    "    assert mask.dtype == np.uint8, f\"Training mask dtype is {mask.dtype}\"\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=30,\n",
    "            layers='heads')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "2A43CMnpl9Z0",
    "outputId": "67e1627c-3c61-41bf-cc30-a428f4d23bb7"
   },
   "outputs": [],
   "source": [
    "history_path = os.path.join('/content', 'training_logs_model.txt')\n",
    "with open(history_path, 'w') as log_file:\n",
    "    log_file.write(str(model.keras_model.history.history))  \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = model.keras_model.history.history['loss']\n",
    "val_loss = model.keras_model.history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r*-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passer en mode inférence\n",
    "class InferenceConfig(Config):\n",
    "    NAME = \"Baby\"\n",
    "    NUM_CLASSES = 1 + 2\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    BATCH_SIZE = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "\n",
    "model = MaskRCNN(mode=\"inference\", config=inference_config, model_dir='/content/logs')\n",
    "\n",
    "import os\n",
    "if os.path.exists('/content/logs/mask_rcnn_baby.h5'):\n",
    "  model.load_weights('/content/logs/mask_rcnn_baby.h5', by_name=True)\n",
    "else:\n",
    "  print(\"Weight file not found. Please ensure the model has been trained and the weights are saved to the correct location.\")\n",
    "  \n",
    "\n",
    "# Charger une image pour effectuer des prédictions\n",
    "image_id = dataset_val.image_ids[0]\n",
    "image = dataset_val.load_image(image_id)\n",
    "mask, class_ids = dataset_val.load_mask(image_id)\n",
    "\n",
    "# Effectuer la prédiction\n",
    "results = model.detect([image], verbose=1)\n",
    "r = results[0]\n",
    "\n",
    "# Afficher les résultats\n",
    "import mrcnn.visualize\n",
    "mrcnn.visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n",
    "                                  dataset_val.class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/content/images/\"\n",
    "\n",
    "# Charger le dataset d'entraînement\n",
    "dataset_test = BabyDataset()\n",
    "dataset_test.load_baby( dataset_dir, 'test')\n",
    "dataset_test.prepare()\n",
    "print(f\"Nombre d'images dans le dataset de test: {len(dataset_test.image_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "class InferenceConfig(Config):\n",
    "    NAME = \"Baby\"\n",
    "    NUM_CLASSES = 1 + 2\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "inference_model = MaskRCNN(mode=\"inference\", config=inference_config, model_dir='/content')\n",
    "\n",
    "\n",
    "inference_model.load_weights(model.find_last(), by_name=True)\n",
    "\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for image_id in dataset_test.image_ids:\n",
    "    \n",
    "    true_masks, true_classes = dataset_test.load_mask(image_id)\n",
    "\n",
    "    \n",
    "    image = dataset_test.load_image(image_id)\n",
    "    pred = inference_model.detect([image], verbose=0)[0]\n",
    "\n",
    "    \n",
    "    if len(true_classes) == len(pred['class_ids']):\n",
    "        true_labels.extend(true_classes)\n",
    "        pred_labels.extend(pred['class_ids'])\n",
    "    else:\n",
    "        \n",
    "        true_labels.extend(true_classes)\n",
    "        pred_labels.extend(pred['class_ids'])\n",
    "\n",
    "       \n",
    "        if len(true_classes) > len(pred['class_ids']):\n",
    "            pred_labels.extend([0] * (len(true_classes) - len(pred['class_ids'])))\n",
    "        elif len(pred['class_ids']) > len(true_classes):\n",
    "            true_labels.extend([0] * (len(pred['class_ids']) - len(true_classes)))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1])  \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['background', 'baby'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn import visualize\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "for i in random.sample(list(dataset_test.image_ids), 5):\n",
    "    image = dataset_test.load_image(i)  \n",
    "    results = inference_model.detect([image], verbose=0)  \n",
    "\n",
    "    \n",
    "    visualize.display_instances(\n",
    "        image,\n",
    "        results[0]['rois'],\n",
    "        results[0]['masks'],\n",
    "        results[0]['class_ids'],\n",
    "        dataset_test.class_names,\n",
    "        results[0]['scores']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.utils import compute_ap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "APs = []\n",
    "\n",
    "\n",
    "for image_id in dataset_test.image_ids:\n",
    "    \n",
    "    image = dataset_test.load_image(image_id)\n",
    "\n",
    "    \n",
    "    gt_mask, gt_class_ids = dataset_test.load_mask(image_id)\n",
    "\n",
    "    \n",
    "    gt_bbox = utils.extract_bboxes(gt_mask)\n",
    "\n",
    "    \n",
    "    results = inference_model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    \n",
    "    if r['masks'].size == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    pred_masks = r['masks']\n",
    "\n",
    "    \n",
    "    if pred_masks.shape[0] != gt_mask.shape[0] or pred_masks.shape[1] != gt_mask.shape[1]:\n",
    "        pred_masks_resized = []\n",
    "        for i in range(pred_masks.shape[-1]):\n",
    "            mask = pred_masks[:, :, i]\n",
    "            mask_resized = cv2.resize(mask.astype(np.uint8),\n",
    "                                    (gt_mask.shape[1], gt_mask.shape[0]),\n",
    "                                    interpolation=cv2.INTER_NEAREST)\n",
    "            pred_masks_resized.append(mask_resized)\n",
    "        pred_masks = np.stack(pred_masks_resized, axis=-1)\n",
    "\n",
    "    \n",
    "    if gt_class_ids.size == 0 or r['class_ids'].size == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    AP, precisions, recalls, overlaps = compute_ap(\n",
    "        gt_bbox,          # Ground truth bounding boxes\n",
    "        gt_class_ids,     # Ground truth class IDs\n",
    "        gt_mask,          # Ground truth masks\n",
    "        r['rois'],        # Predicted bounding boxes\n",
    "        r['class_ids'],   # Predicted class IDs\n",
    "        r['scores'],      # Predicted scores (was missing in original)\n",
    "        pred_masks        # Predicted masks\n",
    "    )\n",
    "\n",
    "    APs.append(AP)\n",
    "\n",
    "\n",
    "if len(APs) > 0:\n",
    "    mAP = np.mean(APs)\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "else:\n",
    "    print(\"No valid predictions found for evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.utils import compute_ap\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import cv2\n",
    "\n",
    "\n",
    "APs = []\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "\n",
    "for image_id in dataset_test.image_ids:\n",
    "    \n",
    "    image = dataset_test.load_image(image_id)\n",
    "    \n",
    "    gt_mask, gt_class_ids = dataset_test.load_mask(image_id)\n",
    "\n",
    "    gt_bbox = utils.extract_bboxes(gt_mask)\n",
    "\n",
    "    \n",
    "    results = inference_model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    \n",
    "    if r['masks'].size == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    pred_masks = r['masks']\n",
    "\n",
    "    \n",
    "    if pred_masks.shape[0] != gt_mask.shape[0] or pred_masks.shape[1] != gt_mask.shape[1]:\n",
    "        pred_masks_resized = []\n",
    "        for i in range(pred_masks.shape[-1]):\n",
    "            mask = pred_masks[:, :, i]\n",
    "            mask_resized = cv2.resize(mask.astype(np.uint8),\n",
    "                                    (gt_mask.shape[1], gt_mask.shape[0]),\n",
    "                                    interpolation=cv2.INTER_NEAREST)\n",
    "            pred_masks_resized.append(mask_resized)\n",
    "        pred_masks = np.stack(pred_masks_resized, axis=-1)\n",
    "\n",
    "    \n",
    "    if gt_class_ids.size == 0 or r['class_ids'].size == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    AP, precisions, recalls, overlaps = compute_ap(\n",
    "        gt_bbox,          # Ground truth bounding boxes\n",
    "        gt_class_ids,     # Ground truth class IDs\n",
    "        gt_mask,          # Ground truth masks\n",
    "        r['rois'],        # Predicted bounding boxes\n",
    "        r['class_ids'],   # Predicted class IDs\n",
    "        r['scores'],      # Predicted scores\n",
    "        pred_masks        # Predicted masks\n",
    "    )\n",
    "\n",
    "    APs.append(AP)\n",
    "\n",
    "    \n",
    "    gt_mask_binary = np.any(gt_mask > 0, axis=2).astype(np.int32)\n",
    "    pred_mask_binary = np.any(pred_masks > 0, axis=2).astype(np.int32)\n",
    "\n",
    "    \n",
    "    gt_mask_flat = gt_mask_binary.flatten()\n",
    "    pred_mask_flat = pred_mask_binary.flatten()\n",
    "\n",
    "   \n",
    "    all_true_labels.extend(gt_mask_flat)\n",
    "    all_pred_labels.extend(pred_mask_flat)\n",
    "\n",
    "\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "all_pred_labels = np.array(all_pred_labels)\n",
    "\n",
    "\n",
    "if len(APs) > 0:\n",
    "    \n",
    "    mAP = np.mean(APs)\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "\n",
    "    \n",
    "    precision = precision_score(all_true_labels, all_pred_labels, average='binary')\n",
    "    recall = recall_score(all_true_labels, all_pred_labels, average='binary')\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels, average='binary')\n",
    "\n",
    "   \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    \n",
    "    print(\"\\nPer-image AP Statistics:\")\n",
    "    print(f\"Min AP: {np.min(APs):.4f}\")\n",
    "    print(f\"Max AP: {np.max(APs):.4f}\")\n",
    "    print(f\"Median AP: {np.median(APs):.4f}\")\n",
    "    print(f\"Standard Deviation AP: {np.std(APs):.4f}\")\n",
    "\n",
    "    \n",
    "    percentiles = [25, 50, 75, 90, 95]\n",
    "    print(\"\\nAP Percentiles:\")\n",
    "    for p in percentiles:\n",
    "        print(f\"{p}th percentile: {np.percentile(APs, p):.4f}\")\n",
    "else:\n",
    "    print(\"No valid predictions found for evaluation\")\n",
    "\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "metrics = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"mAP\": float(mAP),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"num_images_evaluated\": len(APs),\n",
    "    \"ap_statistics\": {\n",
    "        \"min\": float(np.min(APs)),\n",
    "        \"max\": float(np.max(APs)),\n",
    "        \"median\": float(np.median(APs)),\n",
    "        \"std\": float(np.std(APs)),\n",
    "        \"percentiles\": {\n",
    "            str(p): float(np.percentile(APs, p)) for p in percentiles\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "with open('evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(\"\\nMetrics saved to 'evaluation_metrics.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Construire le chemin de sauvegarde\n",
    "output_dir = \"/content/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # S'assure que le dossier existe\n",
    "output_file = os.path.join(output_dir, \"evaluation_metrics_model.json\")\n",
    "\n",
    "# Construire les métriques\n",
    "metrics = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"mAP\": float(mAP),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"num_images_evaluated\": len(APs),\n",
    "    \"ap_statistics\": {\n",
    "        \"min\": float(np.min(APs)),\n",
    "        \"max\": float(np.max(APs)),\n",
    "        \"median\": float(np.median(APs)),\n",
    "        \"std\": float(np.std(APs)),\n",
    "        \"percentiles\": {\n",
    "            str(p): float(np.percentile(APs, p)) for p in percentiles\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder les métriques dans un fichier JSON\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"\\nMetrics saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "application fonction detection position du bébé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import skimage.io\n",
    "\n",
    "def detect_baby_orientation(image_path, model):\n",
    "    \"\"\"Détecte l'orientation d'un bébé (face ou dos) dans une image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Chemin de l'image à analyser.\n",
    "        model (MaskRCNN): Modèle Mask R-CNN entraîné.\n",
    "\n",
    "    Returns:\n",
    "        str: \"face\" si le bébé est de face, \"dos\" si le bébé est de dos, \"inconnu\" sinon.\n",
    "    \"\"\"\n",
    "\n",
    "    # Charger l'image\n",
    "    image = skimage.io.imread(image_path)\n",
    "\n",
    "    # Faire une prédiction avec le modèle\n",
    "    results = inference_model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    # Vérifier les classes détectées\n",
    "    class_ids = r['class_ids']\n",
    "\n",
    "    # Définir les IDs de classe pour \"face\" et \"nose\"\n",
    "    face_id = 1 # Assurez-vous que c'est l'ID de classe correct pour \"face\"\n",
    "    nose_id = 2 # Assurez-vous que c'est l'ID de classe correct pour \"nose\"\n",
    "\n",
    "    if face_id in class_ids or nose_id in class_ids:\n",
    "        return \"face\"  # Bébé de face si \"face\" ou \"nose\" sont détectés\n",
    "    elif face_id in class_ids:\n",
    "        return \"coté\"   # Bébé de dos si seulement \"face\" est détecté\n",
    "    else:\n",
    "        return \"dos\"  # Orientation inconnue si ni \"face\" ni \"nose\" ne sont détectés\n",
    "\n",
    "\n",
    "# Chemin du dossier contenant les images de test\n",
    "test_images_dir = '/content/images/test'\n",
    "\n",
    "# Parcourir les images de test et détecter l'orientation du bébé\n",
    "for filename in os.listdir(test_images_dir):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):  # Filtrer les fichiers image\n",
    "        image_path = os.path.join(test_images_dir, filename)\n",
    "        orientation = detect_baby_orientation(image_path, model)\n",
    "\n",
    "        print(f\"Image: {filename}, Orientation: {orientation}\")\n",
    "\n",
    "        # Afficher l'image avec l'orientation détectée (optionnel)\n",
    "        image = skimage.io.imread(image_path)\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(image)\n",
    "\n",
    "        # Ajouter une annotation pour l'orientation\n",
    "        ax.text(10, 30, orientation, color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PFXhUlmcnbe"
   },
   "source": [
    " # création du model et entrainement avec la deuxième configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy70ff_uNCe_"
   },
   "outputs": [],
   "source": [
    "from mrcnn.model import MaskRCNN\n",
    "config = BabyConfig_1()\n",
    "# Créer le modèle en mode entraînement\n",
    "model_1 = MaskRCNN(mode=\"training\", config=config, model_dir='/content/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMDJT2OeNIXc",
    "outputId": "850ce643-af30-46f6-f0e5-4ad59d427aa3"
   },
   "outputs": [],
   "source": [
    "# Charger un modèle pré-entraîné (par exemple COCO)\n",
    "model_1.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=[\n",
    "    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "\n",
    "# Vérifier les types de données avant l'entraînement\n",
    "for image_id in dataset_train.image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    assert image.dtype == np.uint8, f\"Training image dtype is {image.dtype}\"\n",
    "    assert mask.dtype == np.uint8, f\"Training mask dtype is {mask.dtype}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8craSnSopZln",
    "outputId": "a8837192-b19b-405d-be9f-7d753f6c6499"
   },
   "outputs": [],
   "source": [
    "# Entraîner le modèle\n",
    "model_1.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=75,\n",
    "            layers='heads')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "RI4MPzRdQSyF",
    "outputId": "e477798e-793e-41cf-ac78-a940ed9fc40e"
   },
   "outputs": [],
   "source": [
    "history_path = os.path.join('/content', 'training_logs_model_1.txt')\n",
    "with open(history_path, 'w') as log_file:\n",
    "    log_file.write(str(model_1.keras_model.history.history))  \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = model_1.keras_model.history.history['loss']\n",
    "val_loss = model_1.keras_model.history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r*-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32cD6Vqg9Arm"
   },
   "source": [
    "code inférence pour la deuxieme config des hyperparametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "id": "HwvG9FT680Bn",
    "outputId": "f8b987af-89d9-40c1-9dd4-2db5098d5f1d"
   },
   "outputs": [],
   "source": [
    "# Passer en mode inférence\n",
    "inference_config = CustomConfig()\n",
    "inference_config.GPU_COUNT = 1\n",
    "inference_config.IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config.BATCH_SIZE = 1 \n",
    "\n",
    "\n",
    "model_1 = MaskRCNN(mode=\"inference\", config=inference_config, model_dir='/content/logs')\n",
    "\n",
    "import os\n",
    "if os.path.exists('/content/logs/mask_rcnn_baby.h5'):\n",
    "  model_1.load_weights('/content/logs/mask_rcnn_baby.h5', by_name=True)\n",
    "else:\n",
    "  print(\"Weight file not found. Please ensure the model has been trained and the weights are saved to the correct location.\")\n",
    "  \n",
    "\n",
    "# Charger une image pour effectuer des prédictions\n",
    "image_id = dataset_val.image_ids[0]\n",
    "image = dataset_val.load_image(image_id)\n",
    "mask, class_ids = dataset_val.load_mask(image_id)\n",
    "\n",
    "# Effectuer la prédiction\n",
    "results = model_1.detect([image], verbose=1)\n",
    "r = results[0]\n",
    "\n",
    "# Afficher les résultats\n",
    "import mrcnn.visualize\n",
    "mrcnn.visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n",
    "                                  dataset_val.class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CR5-LVuL9Ieb",
    "outputId": "9fd96250-92e1-479b-d837-4828f077752c"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "class InferenceConfig(Config):\n",
    "    NAME = \"Baby\"\n",
    "    NUM_CLASSES = 1 + 2  \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "inference_model_1 = MaskRCNN(mode=\"inference\", config=inference_config, model_dir='/content')\n",
    "\n",
    "\n",
    "inference_model_1.load_weights(model_1.find_last(), by_name=True)\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for image_id in dataset_test.image_ids:\n",
    "    \n",
    "    true_masks, true_classes = dataset_test.load_mask(image_id)\n",
    "\n",
    "    \n",
    "    image = dataset_test.load_image(image_id)\n",
    "    pred = inference_model_1.detect([image], verbose=0)[0]\n",
    "\n",
    "    \n",
    "    if len(true_classes) == len(pred['class_ids']):\n",
    "        true_labels.extend(true_classes)\n",
    "        pred_labels.extend(pred['class_ids'])\n",
    "    else:\n",
    "        \n",
    "        true_labels.extend(true_classes)\n",
    "        pred_labels.extend(pred['class_ids'])\n",
    "\n",
    "       \n",
    "        if len(true_classes) > len(pred['class_ids']):\n",
    "            pred_labels.extend([0] * (len(true_classes) - len(pred['class_ids'])))\n",
    "        elif len(pred['class_ids']) > len(true_classes):\n",
    "            true_labels.extend([0] * (len(pred['class_ids']) - len(true_classes)))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels, labels=[0, 1]) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['background', 'baby'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn import visualize\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "for i in random.sample(list(dataset_test.image_ids), 5):\n",
    "    image = dataset_test.load_image(i)  \n",
    "    results = inference_model_1.detect([image], verbose=0)  \n",
    "\n",
    "    \n",
    "    visualize.display_instances(\n",
    "        image,\n",
    "        results[0]['rois'],\n",
    "        results[0]['masks'],\n",
    "        results[0]['class_ids'],\n",
    "        dataset_test.class_names,\n",
    "        results[0]['scores']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZC5UPo739gP7",
    "outputId": "a509815b-ddf5-4c06-a927-5b7bcfe6fa3e"
   },
   "outputs": [],
   "source": [
    "from mrcnn.utils import compute_ap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "APs = []\n",
    "\n",
    "\n",
    "for image_id in dataset_test.image_ids:\n",
    "    \n",
    "    image = dataset_test.load_image(image_id)\n",
    "\n",
    "    \n",
    "    gt_mask, gt_class_ids = dataset_test.load_mask(image_id)\n",
    "\n",
    "    \n",
    "    gt_bbox = utils.extract_bboxes(gt_mask)\n",
    "\n",
    "    \n",
    "    results = inference_model_1.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    \n",
    "    if r['masks'].size == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    pred_masks = r['masks']\n",
    "\n",
    "    \n",
    "    if pred_masks.shape[0] != gt_mask.shape[0] or pred_masks.shape[1] != gt_mask.shape[1]:\n",
    "        pred_masks_resized = []\n",
    "        for i in range(pred_masks.shape[-1]):\n",
    "            mask = pred_masks[:, :, i]\n",
    "            mask_resized = cv2.resize(mask.astype(np.uint8),\n",
    "                                    (gt_mask.shape[1], gt_mask.shape[0]),\n",
    "                                    interpolation=cv2.INTER_NEAREST)\n",
    "            pred_masks_resized.append(mask_resized)\n",
    "        pred_masks = np.stack(pred_masks_resized, axis=-1)\n",
    "\n",
    "    \n",
    "    if gt_class_ids.size == 0 or r['class_ids'].size == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    AP, precisions, recalls, overlaps = compute_ap(\n",
    "        gt_bbox,          # Ground truth bounding boxes\n",
    "        gt_class_ids,     # Ground truth class IDs\n",
    "        gt_mask,          # Ground truth masks\n",
    "        r['rois'],        # Predicted bounding boxes\n",
    "        r['class_ids'],   # Predicted class IDs\n",
    "        r['scores'],      # Predicted scores (was missing in original)\n",
    "        pred_masks        # Predicted masks\n",
    "    )\n",
    "\n",
    "    APs.append(AP)\n",
    "\n",
    "\n",
    "if len(APs) > 0:\n",
    "    mAP = np.mean(APs)\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "else:\n",
    "    print(\"No valid predictions found for evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIGXv6we9rAG",
    "outputId": "89f89f97-8055-427f-a094-034593dfef30"
   },
   "outputs": [],
   "source": [
    "from mrcnn.utils import compute_ap\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import cv2\n",
    "\n",
    "\n",
    "APs = []\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "\n",
    "for image_id in dataset_test.image_ids:\n",
    "    \n",
    "    image = dataset_test.load_image(image_id)\n",
    "\n",
    "    \n",
    "    gt_mask, gt_class_ids = dataset_test.load_mask(image_id)\n",
    "\n",
    "    \n",
    "    gt_bbox = utils.extract_bboxes(gt_mask)\n",
    "\n",
    "    \n",
    "    results = inference_model_1.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    \n",
    "    if r['masks'].size == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    pred_masks = r['masks']\n",
    "\n",
    "    \n",
    "    if pred_masks.shape[0] != gt_mask.shape[0] or pred_masks.shape[1] != gt_mask.shape[1]:\n",
    "        pred_masks_resized = []\n",
    "        for i in range(pred_masks.shape[-1]):\n",
    "            mask = pred_masks[:, :, i]\n",
    "            mask_resized = cv2.resize(mask.astype(np.uint8),\n",
    "                                    (gt_mask.shape[1], gt_mask.shape[0]),\n",
    "                                    interpolation=cv2.INTER_NEAREST)\n",
    "            pred_masks_resized.append(mask_resized)\n",
    "        pred_masks = np.stack(pred_masks_resized, axis=-1)\n",
    "\n",
    "    \n",
    "    if gt_class_ids.size == 0 or r['class_ids'].size == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    AP, precisions, recalls, overlaps = compute_ap(\n",
    "        gt_bbox,          # Ground truth bounding boxes\n",
    "        gt_class_ids,     # Ground truth class IDs\n",
    "        gt_mask,          # Ground truth masks\n",
    "        r['rois'],        # Predicted bounding boxes\n",
    "        r['class_ids'],   # Predicted class IDs\n",
    "        r['scores'],      # Predicted scores\n",
    "        pred_masks        # Predicted masks\n",
    "    )\n",
    "\n",
    "    APs.append(AP)\n",
    "\n",
    "    \n",
    "    gt_mask_binary = np.any(gt_mask > 0, axis=2).astype(np.int32)\n",
    "    pred_mask_binary = np.any(pred_masks > 0, axis=2).astype(np.int32)\n",
    "\n",
    "    \n",
    "    gt_mask_flat = gt_mask_binary.flatten()\n",
    "    pred_mask_flat = pred_mask_binary.flatten()\n",
    "\n",
    "    \n",
    "    all_true_labels.extend(gt_mask_flat)\n",
    "    all_pred_labels.extend(pred_mask_flat)\n",
    "\n",
    "\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "all_pred_labels = np.array(all_pred_labels)\n",
    "\n",
    "\n",
    "if len(APs) > 0:\n",
    "    \n",
    "    mAP = np.mean(APs)\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "\n",
    "    \n",
    "    precision = precision_score(all_true_labels, all_pred_labels, average='binary')\n",
    "    recall = recall_score(all_true_labels, all_pred_labels, average='binary')\n",
    "    f1 = f1_score(all_true_labels, all_pred_labels, average='binary')\n",
    "\n",
    "    \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    \n",
    "    print(\"\\nPer-image AP Statistics:\")\n",
    "    print(f\"Min AP: {np.min(APs):.4f}\")\n",
    "    print(f\"Max AP: {np.max(APs):.4f}\")\n",
    "    print(f\"Median AP: {np.median(APs):.4f}\")\n",
    "    print(f\"Standard Deviation AP: {np.std(APs):.4f}\")\n",
    "\n",
    "    \n",
    "    percentiles = [25, 50, 75, 90, 95]\n",
    "    print(\"\\nAP Percentiles:\")\n",
    "    for p in percentiles:\n",
    "        print(f\"{p}th percentile: {np.percentile(APs, p):.4f}\")\n",
    "else:\n",
    "    print(\"No valid predictions found for evaluation\")\n",
    "\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "metrics = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"mAP\": float(mAP),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"num_images_evaluated\": len(APs),\n",
    "    \"ap_statistics\": {\n",
    "        \"min\": float(np.min(APs)),\n",
    "        \"max\": float(np.max(APs)),\n",
    "        \"median\": float(np.median(APs)),\n",
    "        \"std\": float(np.std(APs)),\n",
    "        \"percentiles\": {\n",
    "            str(p): float(np.percentile(APs, p)) for p in percentiles\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics to JSON file\n",
    "with open('evaluation_metrics_1.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(\"\\nMetrics saved to 'evaluation_metrics_1.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-T_jwFH99S-",
    "outputId": "8b105cef-eeba-4caa-b118-2e7a622eda32"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Construire le chemin de sauvegarde\n",
    "output_dir = \"/content/images\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # S'assure que le dossier existe\n",
    "output_file = os.path.join(output_dir, \"evaluation_metrics_model_1.json\")\n",
    "\n",
    "# Construire les métriques\n",
    "metrics = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"mAP\": float(mAP),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"num_images_evaluated\": len(APs),\n",
    "    \"ap_statistics\": {\n",
    "        \"min\": float(np.min(APs)),\n",
    "        \"max\": float(np.max(APs)),\n",
    "        \"median\": float(np.median(APs)),\n",
    "        \"std\": float(np.std(APs)),\n",
    "        \"percentiles\": {\n",
    "            str(p): float(np.percentile(APs, p)) for p in percentiles\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder les métriques dans un fichier JSON\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"\\nMetrics saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonction qui permet la detction de la position du bébé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "loLK58uz-DrE",
    "outputId": "d902f828-4d48-4a04-9a10-425eb14284d7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import skimage.io\n",
    "\n",
    "def detect_baby_orientation(image_path, model):\n",
    "    \"\"\"Détecte l'orientation d'un bébé (face ou dos) dans une image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Chemin de l'image à analyser.\n",
    "        model (MaskRCNN): Modèle Mask R-CNN entraîné.\n",
    "\n",
    "    Returns:\n",
    "        str: \"face\" si le bébé est de face, \"dos\" si le bébé est de dos, \"inconnu\" sinon.\n",
    "    \"\"\"\n",
    "\n",
    "    # Charger l'image\n",
    "    image = skimage.io.imread(image_path)\n",
    "\n",
    "    # Faire une prédiction avec le modèle\n",
    "    results = inference_model_1.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    # Vérifier les classes détectées\n",
    "    class_ids = r['class_ids']\n",
    "\n",
    "    # Définir les IDs de classe pour \"face\" et \"nose\"\n",
    "    face_id = 1 # Assurez-vous que c'est l'ID de classe correct pour \"face\"\n",
    "    nose_id = 2 # Assurez-vous que c'est l'ID de classe correct pour \"nose\"\n",
    "\n",
    "    if face_id in class_ids or nose_id in class_ids:\n",
    "        return \"face\"  # Bébé de face si \"face\" et \"nose\" sont détectés\n",
    "    elif face_id in class_ids:\n",
    "        return \"coté\"   # Bébé de dos si seulement \"face\" est détecté\n",
    "    else:\n",
    "        return \"dos\"  # Orientation inconnue si ni \"face\" ni \"nose\" ne sont détectés\n",
    "\n",
    "\n",
    "# Chemin du dossier contenant les images de test\n",
    "test_images_dir = '/content/images/test'\n",
    "\n",
    "# Parcourir les images de test et détecter l'orientation du bébé\n",
    "for filename in os.listdir(test_images_dir):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):  # Filtrer les fichiers image\n",
    "        image_path = os.path.join(test_images_dir, filename)\n",
    "        orientation = detect_baby_orientation(image_path, model_1)\n",
    "\n",
    "        print(f\"Image: {filename}, Orientation: {orientation}\")\n",
    "\n",
    "        # Afficher l'image avec l'orientation détectée (optionnel)\n",
    "        image = skimage.io.imread(image_path)\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(image)\n",
    "\n",
    "        # Ajouter une annotation pour l'orientation\n",
    "        ax.text(10, 30, orientation, color='red', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec application sur une video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6NRb35RGn2y5",
    "outputId": "fd27c943-d043-4f1a-8d06-8e16628beb5c"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "def detect_baby_orientation(image, model):\n",
    "    \"\"\"\n",
    "    Détecte l'orientation du bébé dans une image.\n",
    "    Retourne l'orientation détectée et les résultats du modèle (bounding boxes, etc.).\n",
    "    \"\"\"\n",
    "    # Convertir l'image de BGR à RGB pour le modèle Mask R-CNN\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Faire une prédiction avec le modèle\n",
    "    results = model.detect([rgb_image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    # Ici vous pouvez définir la logique d'orientation de votre bébé en fonction des classes détectées.\n",
    "    # (Déterminez l'orientation selon les résultats du modèle)\n",
    "    class_ids = r['class_ids']\n",
    "\n",
    "    face_id = 1  # Exemple d'ID de classe pour \"face\"\n",
    "    nose_id = 2  # Exemple d'ID de classe pour \"nose\"\n",
    "\n",
    "    if face_id in class_ids or nose_id in class_ids:\n",
    "        orientation = \"face\"  # Bébé de face\n",
    "    elif face_id in class_ids:\n",
    "        orientation = \"dos\"  # Bébé de dos\n",
    "    else:\n",
    "        orientation = \"inconnu\"  # Orientation inconnue\n",
    "\n",
    "    return orientation, r\n",
    "\n",
    "def process_video(video_path, model, output_path=None):\n",
    "    \"\"\"\n",
    "    Traite une vidéo pour détecter la position du bébé image par image.\n",
    "    Enregistre la vidéo de sortie avec les annotations (bounding boxes et orientation).\n",
    "    \"\"\"\n",
    "    # Ouvrir la vidéo\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Vérifier si la vidéo s'ouvre correctement\n",
    "    if not cap.isOpened():\n",
    "        print(\"Erreur : Impossible d'ouvrir la vidéo.\")\n",
    "        return\n",
    "\n",
    "    # Obtenir les dimensions et le FPS de la vidéo\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Définir le writer si un fichier de sortie est spécifié\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec pour le fichier de sortie\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convertir l'image de BGR à RGB pour l'input du modèle\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Détecter l'orientation du bébé\n",
    "        orientation, results = detect_baby_orientation(rgb_frame, model)\n",
    "\n",
    "        # Dessiner les bounding boxes et afficher l'orientation\n",
    "        for i, bbox in enumerate(results['rois']):\n",
    "            y1, x1, y2, x2 = bbox\n",
    "            label = orientation\n",
    "            color = (0, 255, 0) if label == \"face\" else (0, 0, 255)\n",
    "\n",
    "            # Dessiner la bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            # Ajouter le texte pour l'orientation\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Afficher la frame dans Colab\n",
    "        cv2_imshow(frame)\n",
    "\n",
    "        # Enregistrer dans le fichier de sortie si nécessaire\n",
    "        if output_path:\n",
    "            out.write(frame)\n",
    "\n",
    "        # Arrêter avec la touche 'q' (nécessaire uniquement pour exécutions locales)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Libérer les ressources\n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Chemin de la vidéo d'entrée et du fichier de sortie\n",
    "video_input_path = '/content/video.mp4'\n",
    "video_output_path = '/content/output_video.mp4'\n",
    "\n",
    "# Traiter la vidéo avec le modèle\n",
    "process_video(video_input_path, inference_model_1, output_path=video_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui permet de determiner la position du bébé sur une vidéo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "61sEc3FTl9zl",
    "outputId": "bf74cbb5-30fa-4cc5-b1fe-8d7ae4c0ddc0"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Import cv2_imshow from google.colab.patches\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# ... (other functions remain the same) ...\n",
    "\n",
    "def process_video(video_path, model, output_path=None):\n",
    "    \"\"\"\n",
    "    Traite une vidéo pour détecter la position du bébé image par image.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Chemin de la vidéo à analyser.\n",
    "        model (MaskRCNN): Modèle Mask R-CNN entraîné.\n",
    "        output_path (str): Chemin du fichier vidéo de sortie (facultatif).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ouvrir la vidéo\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Vérifier si la vidéo s'ouvre correctement\n",
    "    if not cap.isOpened():\n",
    "        print(\"Erreur : Impossible d'ouvrir la vidéo.\")\n",
    "        return\n",
    "\n",
    "    # Obtenir les dimensions de la vidéo\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Définir le writer si un fichier de sortie est spécifié\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec pour le fichier de sortie\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convertir l'image de BGR (par défaut OpenCV) à RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Détecter l'orientation du bébé\n",
    "        orientation, results = detect_baby_orientation(rgb_frame, model)\n",
    "\n",
    "        # Dessiner les bounding boxes et afficher l'orientation\n",
    "        for i, bbox in enumerate(results['rois']):\n",
    "            y1, x1, y2, x2 = bbox\n",
    "            label = orientation\n",
    "            color = (0, 255, 0) if label == \"face\" else (0, 0, 255)\n",
    "\n",
    "            # Dessiner la bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            # Ajouter le texte pour l'orientation\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Afficher la frame using cv2_imshow()\n",
    "        cv2_imshow(frame)\n",
    "\n",
    "        # Enregistrer dans le fichier de sortie si nécessaire\n",
    "        if output_path:\n",
    "            out.write(frame)\n",
    "\n",
    "        # Quitter avec la touche 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Libérer les ressources\n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Chemin de la vidéo d'entrée et du fichier de sortie\n",
    "video_input_path = '/content/video.mp4'\n",
    "video_output_path = '/content/output_video.mp4'\n",
    "\n",
    "# Traiter la vidéo avec le modèle\n",
    "process_video(video_input_path, inference_model_1, output_path=video_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTJXjRK2quiZ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "LXimL0RTqucB",
    "outputId": "bb999957-7f66-4071-f052-eafec2dbb2f0"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "def process_video(video_path, model, output_path=None):\n",
    "    \"\"\"\n",
    "    Traite une vidéo pour détecter la position du bébé image par image.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Chemin de la vidéo à analyser.\n",
    "        model (MaskRCNN): Modèle Mask R-CNN entraîné.\n",
    "        output_path (str): Chemin du fichier vidéo de sortie (facultatif).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ouvrir la vidéo\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Vérifier si la vidéo s'ouvre correctement\n",
    "    if not cap.isOpened():\n",
    "        print(\"Erreur : Impossible d'ouvrir la vidéo.\")\n",
    "        return\n",
    "\n",
    "    # Obtenir les dimensions et le FPS de la vidéo\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Définir le writer si un fichier de sortie est spécifié\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec pour le fichier de sortie\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convertir l'image de BGR (par défaut OpenCV) à RGB pour le modèle\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Détecter l'orientation du bébé\n",
    "        orientation, results = detect_baby_orientation(rgb_frame, model)\n",
    "\n",
    "        # Dessiner les bounding boxes et afficher l'orientation\n",
    "        for i, bbox in enumerate(results['rois']):\n",
    "            y1, x1, y2, x2 = bbox\n",
    "            label = orientation\n",
    "            color = (0, 255, 0) if label == \"face\" else (0, 0, 255)\n",
    "\n",
    "            # Dessiner la bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            # Ajouter le texte pour l'orientation\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        # Afficher la frame dans Colab\n",
    "        cv2_imshow(frame)\n",
    "\n",
    "        # Enregistrer dans le fichier de sortie si nécessaire\n",
    "        if output_path:\n",
    "            out.write(frame)\n",
    "\n",
    "        # Arrêter avec la touche 'q' (nécessaire uniquement pour exécutions locales)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Libérer les ressources\n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Chemin de la vidéo d'entrée et du fichier de sortie\n",
    "video_input_path = '/content/video.mp4'\n",
    "video_output_path = '/content/output_video.mp4'\n",
    "\n",
    "# Traiter la vidéo avec le modèle\n",
    "process_video(video_input_path, inference_model_1, output_path=video_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des deux models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première comparaison avec les logs d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlwLxa82quOt"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_logs(file_path):\n",
    "    \"\"\"\n",
    "    Extrait les métriques à partir d'un fichier de logs.\n",
    "    Supposons que chaque ligne de log contient une mention claire de la métrique d'intérêt (par exemple, 'loss', 'accuracy').\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            # Utiliser une regex pour détecter des métriques spécifiques\n",
    "            match = re.search(r\"loss: ([0-9]*\\.[0-9]+)|accuracy: ([0-9]*\\.[0-9]+)\", line)\n",
    "            if match:\n",
    "                loss = float(match.group(1)) if match.group(1) else None\n",
    "                accuracy = float(match.group(2)) if match.group(2) else None\n",
    "                metrics.append({\"loss\": loss, \"accuracy\": accuracy})\n",
    "    return metrics\n",
    "\n",
    "def compare_models(log1, log2, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Compare deux modèles sur une métrique spécifique.\n",
    "    :param log1: Liste de dictionnaires (métriques du modèle 1).\n",
    "    :param log2: Liste de dictionnaires (métriques du modèle 2).\n",
    "    :param metric: La métrique à comparer ('accuracy' ou 'loss').\n",
    "    :return: Le modèle gagnant et les moyennes des métriques.\n",
    "    \"\"\"\n",
    "    model1_values = [entry[metric] for entry in log1 if entry[metric] is not None]\n",
    "    model2_values = [entry[metric] for entry in log2 if entry[metric] is not None]\n",
    "\n",
    "    model1_avg = sum(model1_values) / len(model1_values) if model1_values else float('inf')\n",
    "    model2_avg = sum(model2_values) / len(model2_values) if model2_values else float('inf')\n",
    "\n",
    "    if metric == \"loss\":\n",
    "        winner = \"Model 1\" if model1_avg < model2_avg else \"Model 2\"\n",
    "    else:  # Pour 'accuracy' et autres métriques où un score plus élevé est meilleur\n",
    "        winner = \"Model 1\" if model1_avg > model2_avg else \"Model 2\"\n",
    "\n",
    "    return winner, model1_avg, model2_avg\n",
    "\n",
    "# Chemins des fichiers de logs\n",
    "file1 = \"/content/training_logs_model.txt\"\n",
    "file2 = \"/content/training_logs_model_1.txt\"\n",
    "\n",
    "# Analyse des fichiers\n",
    "logs_model1 = parse_logs(file1)\n",
    "logs_model2 = parse_logs(file2)\n",
    "\n",
    "# Comparaison\n",
    "metric_to_compare = \"accuracy\"  # ou \"loss\"\n",
    "winner, avg_model1, avg_model2 = compare_models(logs_model1, logs_model2, metric=metric_to_compare)\n",
    "\n",
    "print(f\"Métrique comparée : {metric_to_compare}\")\n",
    "print(f\"Moyenne modèle 1 : {avg_model1}\")\n",
    "print(f\"Moyenne modèle 2 : {avg_model2}\")\n",
    "print(f\"Modèle le plus performant : {winner}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deuxième comparaison cette fois avec les métriques d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_metrics(file_path):\n",
    "    \"\"\"\n",
    "    Charge les métriques à partir d'un fichier JSON.\n",
    "    :param file_path: Chemin vers le fichier JSON.\n",
    "    :return: Dictionnaire contenant les métriques.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def compare_metrics(metrics1, metrics2, keys_to_compare=None):\n",
    "    \"\"\"\n",
    "    Compare les métriques de deux modèles.\n",
    "    :param metrics1: Dictionnaire des métriques du modèle 1.\n",
    "    :param metrics2: Dictionnaire des métriques du modèle 2.\n",
    "    :param keys_to_compare: Liste des clés à comparer. Compare tout si None.\n",
    "    :return: Résultat de la comparaison.\n",
    "    \"\"\"\n",
    "    if keys_to_compare is None:\n",
    "        keys_to_compare = [\"mAP\", \"precision\", \"recall\", \"f1_score\"]\n",
    "\n",
    "    results = {}\n",
    "    for key in keys_to_compare:\n",
    "        value1 = metrics1.get(key, None)\n",
    "        value2 = metrics2.get(key, None)\n",
    "\n",
    "        if value1 is None or value2 is None:\n",
    "            results[key] = \"Indisponible pour comparaison\"\n",
    "        else:\n",
    "            # Déterminer le meilleur modèle pour chaque métrique\n",
    "            if value1 > value2:\n",
    "                results[key] = \"Model 1 est meilleur\"\n",
    "            elif value1 < value2:\n",
    "                results[key] = \"Model 2 est meilleur\"\n",
    "            else:\n",
    "                results[key] = \"Égalité\"\n",
    "\n",
    "    return results\n",
    "\n",
    "# Chemins des fichiers JSON\n",
    "file1 = \"/content/evaluation_metrics.json\"\n",
    "file2 = \"/content/evaluation_metrics_1.json\"\n",
    "\n",
    "# Chargement des métriques\n",
    "metrics_model1 = load_metrics(file1)\n",
    "metrics_model2 = load_metrics(file2)\n",
    "\n",
    "# Comparaison\n",
    "comparison_results = compare_metrics(metrics_model1, metrics_model2)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nRésultats de la comparaison :\")\n",
    "for metric, result in comparison_results.items():\n",
    "    print(f\"{metric}: {result}\")\n",
    "\n",
    "# Identifier le modèle globalement meilleur (par exemple, basé sur le F1-score ou mAP)\n",
    "key_priority = \"f1_score\"  # Priorité sur F1-score\n",
    "if metrics_model1[key_priority] > metrics_model2[key_priority]:\n",
    "    print(\"\\nGlobalement, Model 1 est meilleur basé sur le F1-score.\")\n",
    "elif metrics_model1[key_priority] < metrics_model2[key_priority]:\n",
    "    print(\"\\nGlobalement, Model 2 est meilleur basé sur le F1-score.\")\n",
    "else:\n",
    "    print(\"\\nGlobalement, les deux modèles sont équivalents sur le F1-score.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
